 <!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  <meta name="author" content="Sangmin Woo">
  <meta name="description" content="Applied Scientist @ Amazon">
  <link rel="alternate" hreflang="en-us" href="/">
  <meta name="theme-color" content="#2962ff">
   
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css">
  
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="/css/academic.css">
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Sangmin Woo">
  <link rel="manifest" href="/index.webmanifest">

  <link rel="canonical" href="/">
  <meta property="twitter:card" content="summary">
  <meta property="og:site_name" content="Sangmin Woo - Amazon">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Sangmin Woo">
  <meta property="og:description" content="Applied Scientist @ Amazon"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "/"
}
</script>
  <title>Sangmin Woo</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Sangmin Woo</a>
    </div>
    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>

    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Sangmin Woo</a>
    </div>

    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content"> 
      <ul class="navbar-nav d-md-inline-flex">
        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#experiences" data-target="#experiences"><span>Experiences</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#papers" data-target="#papers"><span>Publications</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#others" data-target="#others"><span>Others</span></a>
        </li>

      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>

    </ul>

  </div>
</nav>


<span class="js-widget-page d-none"></span>

<section id="about" class="home-section wg-about" style="padding: 30px 0 20px 0;">
  <div class="container">

    <div class="row">
      <div class="col-12 col-lg-4">
        <div id="profile">
          <img class="avatar avatar-circle" src="/authors/admin/profile.png" alt="Avatar">
          <div class="portrait-title">
            <h2>Sangmin Woo</h2>
            <h3>Applied Scientist @ Amazon</h3>
          </div>
          <ul class="network-icon" aria-hidden="true">
            <li>
              <a href="https://scholar.google.com/citations?user=5hvoV1UAAAAJ" target="_blank" rel="noopener">
                <i class="ai ai-google-scholar"></i> <b>Google Scholar</b>
              </a>
            </li>
            <li>
              <a href="https://github.com/sangminwoo" target="_blank" rel="noopener">
                <i class="fab fa-github"></i> <b>Github</b>
              </a>
            </li>
            <li>
              <a href="cv/CV_Sangmin_Woo_June_2025.pdf" target="_blank" rel="noopener">
                <i class="fas fa-user-circle"></i> <b>CV</b>
              </a>
            </li>
          </ul>
        </div>
      </div>

      <div class="col-12 col-lg-8">
        <br>
        <p>
          I am an Applied Scientist at Amazon Agentic AI. I received my Ph.D. degree at 
          <a href="https://kaist.ac.kr/en" target="_blank" rel="noopener">KAIST</a>.
        </p>
        
        <p>
          Humans are inherently <b>multi-modal learners</b>, naturally understanding the world by looking (vision), 
          listening (audio), and communicating (language). I am passionate about advancing machine intelligence 
          to mirror this ability, enabling systems to understand the world holistically and 
          <b>generate</b> faithful, human-centered content.
        </p>

        <p>My work explores the following, but not limited to:</p>
        <ul>
          <li>
            <b>Multi-modal AI</b>: Vision + {Language, Audio, <i>etc.</i>}  
            <span class="badge badge-success">Multi-modal</span>
          </li>
          <li>
            <b>Generative AI</b> (Vision Language Models, Large Language Models, Diffusion Models) 
            <span class="badge badge-primary">Gen AI</span>
          </li>
          <li>
            <b>Visual Understanding</b>  
            <span class="badge badge-danger">Video</span> 
            <span class="badge badge-warning">Image</span>
          </li>
        </ul>

        <div class="row">
          <div class="col-md-6">
            <h3>Contact</h3>
            <ul class="ul-contact fa-ul">
              <li>
                <i class="fa-li fas fa-envelope"></i>
                <div class="description">
                  <p class="course">sangminw [at] amazon.com</p>
                  <p class="course">shmwoo9395 [at] gmail.com</p>
                  <!-- <p class="course">smwoo95 [at] kaist.ac.kr</p> -->
                </div>
              </li>
              <li>
                <i class="fa-li fas fa-map-marker"></i>
                <div class="description">
                  <p class="course">2795 Augustine Dr, Santa Clara, CA 95054, United States</p>
                  <!-- <p class="course">291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea 34141</p> -->
                </div>
              </li>
            </ul>
          </div>

          <div class="col-md-6">
            <h3>Education</h3>
            <ul class="ul-edu fa-ul">
              <li>
                <i class="fa-li fas fa-graduation-cap"></i>
                <div class="description">
                  <p class="course">Ph.D. @ KAIST, 2025</p>
                  <p class="institution">on "Deep Visual and Multimodal Generation: Advancing Diffusion Models and Large Vision Language Models"</p>
                </div>
              </li>
              <li>
                <i class="fa-li fas fa-graduation-cap"></i>
                <div class="description">
                  <p class="course">M.S. @ GIST, 2021</p>
                  <p class="institution">on "Learning to Detect Visual Relationships in Images and Videos"</p>
                </div>
              </li>
              <li>
                <i class="fa-li fas fa-graduation-cap"></i>
                <div class="description">
                  <p class="course">B.S. @ KNU, 2019</p>
                </div>
              </li>
            </ul>
          </div>
        </div> <!-- end row -->
      </div> <!-- end col-12 col-lg-8 -->
    </div> <!-- end row -->
  </div> <!-- end container -->

  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1>News</h1>
        <!-- Wrap the news items in a scrollable container -->
        <div style="max-height: 200px; overflow-y: auto; padding-right: 1rem;">
          <p>
            <span class="badge badge-light">25.08</span> 1 paper accepted to ESWA! <br>
            <span class="badge badge-light">25.08</span> 1 paper accepted to EMNLP 2025 Main! <br>
            <span class="badge badge-light">25.06</span> I am starting my new chapter at Amazon! üßëüèª‚Äçüíª <br>
            <span class="badge badge-light">25.06</span> Selected as an Outstanding Reviewer at CVPR 2025! <br>
            <span class="badge badge-light">25.05</span> 1 paper accepted to ACL 2025 Findings! <br>
            <span class="badge badge-light">25.05</span> Successfully defended my PhD! üéì<br>
            <span class="badge badge-light">25.04</span> 1 paper accepted to CVIU! <br>
            <span class="badge badge-light">25.02</span> 1 paper accepted to CVPR 2025! <br>
            <span class="badge badge-light">25.01</span> 1 paper accepted to NAACL 2025 Main! <br>
            <span class="badge badge-light">24.12</span> 1 paper accepted to AAAI 2025! <br>
            <span class="badge badge-light">24.09</span> Excited to keep collaborating with the team remotely! <br>
            <span class="badge badge-light">24.09</span> I had a fantastic summer internship with Amazon! <br>
            <span class="badge badge-light">24.07</span> 3 papers accepted to ECCV 2024! <br>
            <span class="badge badge-light">24.06</span> I joined 
              <a href="https://aws.amazon.com/" target="_blank" rel="noopener">Amazon Bedrock</a> as a summer intern!
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section id="experiences" class="home-section wg-experiences" style="padding: 10px 0 10px 0;">
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h1>Experiences</h1>
        <ul>
          <li>
            <div style="float:left"><b>Amazon Agentic AI (Santa Clara, CA, US)</b></div>
            <div style="float:right">June 2025 - Present</div>
            <br>
            Applied Scientist
          </li>
          <li>
            <div style="float:left"><b>Amazon Bedrock (Remote)</b></div>
            <div style="float:right">Sep 2024 - Mar 2025</div>
            <br>
            Applied Scientist Intern
          </li>
          <li>
            <div style="float:left"><b>Amazon Bedrock (Santa Clara, CA, US)</b></div>
            <div style="float:right">Jun 2024 - Sep 2024</div>
            <br>
            Applied Scientist Intern
          </li>
          <li>
            <div style="float:left"><b>NAVER LABS (Suwon, Korea)</b></div>
            <div style="float:right">Apr 2023 - Aug 2023</div>
            <br>
            Research Intern
          </li>
        </ul>
      </div>
    </div>
  </div>
</section>

<section id="papers" class="home-section wg-papers   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      
<div class="row">
  
    <div class="col-lg-12">
      <h1>Publications</h1>
    </div>
    <div class="row">
        <ul class="ul-papers">
        <hr>
        
        <b>2025</b>

          <li>
            <div class="img">
              <img src="/papers/images/2025_eswa_what.png" class="img-responsive" alt="">
            </div>
            <div class="description">
              <p class="authors"><span class="badge badge-success">Multi-modal</span><span class="badge badge-danger">Video</span></p>
              <p class="authors">What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection</span></p>
              <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Junhyug Noh, Kangil Kim</span></p>
              <p class="venue"><span style="color:#626567">ESWA 2025</span> <span style="font-weight:normal; color:#BB2222">(IF=8.6)</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2107.07154">paper</a> |
                <a href="https://github.com/sangminwoo/Temporal-Span-Proposal-Network-VidVRD">code</a>
                ]
              </p>
            </div>
          </li>

          <li>
            <div class="img">
              <img src="/papers/images/2025_emnlp_apo_survey.png" class="img-responsive" alt="">
            </div>
            <div class="description">
              <p class="authors"><span class="badge badge-primary">Gen AI</span></p>
              <p class="authors">A Systematic Survey of Automatic Prompt Optimization Techniques</span></p>
              <p class="authors"><span style="color:#626567">
                Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, <b>Sangmin Woo</b>, Sullam Jeoung,<br>
                Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen,<br>
                Haibo Ding, Panpan Xu, Lin Lee Cheong</span></p>
              <p class="venue"><span style="color:#626567">EMNLP 2025 Main</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2502.16923">paper</a>
                ]
              </p>
            </div>
          </li>

          <li>
            <div class="img">
              <img src="/papers/images/2025_acl_avisc.png" class="img-responsive" alt="">
            </div>
            <div class="description">
              <p class="authors"><span class="badge badge-primary">Gen AI</span><span class="badge badge-success">Multi-modal</span></p>
              <p class="authors">Don‚Äôt Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models</span></p>
              <p class="authors"><span style="color:#626567"><b>Sangmin Woo*</b>, Donguk Kim*, Jaehyuk Jang*, Yubin Choi, Changick Kim (<b>*Equal Contribution</b>)</span></p>
              <p class="venue"><span style="color:#626567">ACL 2025 Findings</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2405.17820">paper</a> |
                <a href="https://github.com/sangminwoo/AvisC">code</a> |
                <a href="https://sangminwoo.github.io/AvisC/">project</a>
                ]
              </p>
            </div>
          </li>

          <li>
            <div class="img">
              <img src="/papers/images/2025_cviu_modality.png" class="img-responsive" alt="">
            </div>
            <div class="description">
              <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
              <p class="authors">Modality Mixer Exploiting Complementary Information for Multi-modal Action Recognition</span></p>
              <p class="authors"><span style="color:#626567">Sumin Lee, <b>Sangmin Woo</b>, Muhammad Adi Nugroho, Changick Kim</span></p>
              <p class="venue"><span style="color:#626567">CVIU 2025</span> <span style="font-weight:normal; color:#BB2222">(IF=4.8)</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2208.11314">paper</a>
                ]
              </p>
            </div>
          </li>

          <li>
            <div class="img">
              <img src="/papers/images/2025_cvpr_prodial.png" class="img-responsive" alt="">
            </div>     
            <div class="description">
              <p class="authors"><span class="badge badge-secondary">Learning</span></p>
              <p class="authors">Parameter Efficient Mamba Tuning via Projector-targeted Diagonal-centric Linear Transformation</span></p>
              <p class="authors"><span style="color:#626567">Seokil Ham, Hee-Seon Kim, <b>Sangmin Woo</b>, Changick Kim</span></p>
              <p class="venue"><span style="color:#626567">CVPR 2025</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2411.15224">paper</a> |
                <a href="https://github.com/SeokilHam/ProDiaL">code</a>
                ]
              </p>
            </div>
          </li>

          <li>
              <div class="img">
                <img src="/papers/images/2025_naacl_bbvpe.png" class="img-responsive" alt="">
              </div>
            <div class="description">
              <p class="authors"><span class="badge badge-primary">Gen AI</span><span class="badge badge-success">Multi-modal</span></p>
              <p class="authors">Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models</span></p>
              <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Kang Zhou, Yun Zhou, Shuai Wang, Sheng Guan, Haibo Ding, Lin Lee Cheong</span></p>
              <p class="venue"><span style="color:#626567">NAACL 2025</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2504.21559">paper</a>
                ]
              </p>
            </div>
          </li>

          <li>
            <div class="img">
              <img src="/papers/images/2025_aaai_dmp.png" class="img-responsive" alt="">
            </div>
            <div class="description">
              <p class="authors"><span class="badge badge-primary">Gen AI</span></p>
              <p class="authors">Diffusion Model Patching via Mixture-of-Prompts</span></p>
              <p class="authors"><span style="color:#626567">Seokil Ham*, <b>Sangmin Woo*</b>, Jinyoung Kim, Hyojun Go, Byeongjun Park, Changick Kim (<b>*Equal Contribution</b>)</span></p>
              <p class="venue"><span style="color:#626567">AAAI 2025</span> </p>
              <p class="resources">
                [
                <a href="https://arxiv.org/abs/2405.17825">paper</a> |
                <a href="https://github.com/sangminwoo/DMP">code</a> |
                <a href="https://sangminwoo.github.io/DMP/">project</a>
                ]
              </p>
            </div>
          </li>

        <hr>

        <b>2024</b>

        <li>
          <div class="img">
            <img src="/papers/images/2024_arxiv_ritual.png" class="img-responsive" alt="">
          </div>
          <div class="description">
            <p class="authors"><span class="badge badge-primary">Gen AI</span><span class="badge badge-success">Multi-modal</span></p>
            <p class="authors">RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo*</b>, Jaehyuk Jang*, Donguk Kim*, Yubin Choi, Changick Kim (<b>*Equal Contribution</b>)</span></p>
            <p class="venue"><span style="color:#626567">Arxiv 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2405.17821">paper</a> |
              <a href="https://github.com/sangminwoo/RITUAL">code</a> |
              <a href="https://sangminwoo.github.io/RITUAL/">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_eccv_flamingnet.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Flow-Assisted Motion Learning Network for Weakly-Supervised Group Activity Recognition</span></p>
            <p class="authors"><span style="color:#626567">Muhammad Adi Nugroho, <b>Sangmin Woo</b>, Sumin Lee, Jinyoung Park, Yooseung Wang, Donguk Kim, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ECCV 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2405.18012">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_eccv_spdpnet.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-danger">Video</span></p>
            <p class="authors">Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition</span></p>
            <p class="authors"><span style="color:#626567">Sumin Lee, Yooseung Wang, <b>Sangmin Woo</b>, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ECCV 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2403.14113">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_eccv_switch_dit.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-primary">Gen AI</span></p>
            <p class="authors">Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts</span></p>
            <p class="authors"><span style="color:#626567">Byeongjun Park, Hyojun Go, Jinyoung Kim, <b>Sangmin Woo</b>, Seokil Ham, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ECCV 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2403.09176">paper</a> |
              <a href="https://github.com/byeongjun-park/Switch-DiT">code</a> |
              <a href="https://byeongjun-park.github.io/Switch-DiT">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_cvpr_harmonyview.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-primary">Gen AI</span></p>
            <p class="authors">HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo*</b>, Byeongjun Park*, Hyojun Go, Jinyoung Kim, Changick Kim (<b>*Equal Contribution</b>)</span></p>
            <p class="venue"><span style="color:#626567">CVPR 2024</span> <br>
              <span style="font-weight:normal; color:#BB2222">Featured by <a href="https://huggingface.co/papers?date=2023-12-27">HuggingFace Daily Papers</a></span> <br>
              <span style="font-weight:normal; color:#BB2222">Finalist, Qualcomm Innovation Fellowship 2024 Korea</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2312.15980">paper</a> |
              <a href="https://github.com/byeongjun-park/HarmonyView">code</a> |
              <a href="https://byeongjun-park.github.io/HarmonyView">project</a> |
              <a href="https://huggingface.co/spaces/byeongjun-park/HarmonyView">demo</a>
              ]
            </p>
          </div>
        </li>
        
        <li>
            <div class="img">
              <img src="/papers/images/2024_iclr_denoising.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-primary">Gen AI</span></p>
            <p class="authors">Denoising Task Routing for Diffusion Models</span></p>
            <p class="authors"><span style="color:#626567">Byeongjun Park*, <b>Sangmin Woo*</b>, Hyojun Go*, Jinyoung Kim*, Changick Kim (<b>*Equal Contribution</b>)</span></p>
            <p class="venue"><span style="color:#626567">ICLR 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2310.07138">paper</a> |
              <a href="https://github.com/byeongjun-park/DTR">code</a> |
              <a href="https://byeongjun-park.github.io/DTR">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_wacv_sketch.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Sketch-based Video Object Localization</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, So-Yeong Jeon, Jinyoung Park, Minji Son, Sumin Lee, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">WACV 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2304.00450">paper</a> |
              <a href="https://github.com/sangminwoo/SVOL">code</a>
              ]
            </p>
          </div>
        </li>

        <hr>
          <b>2023</b>
        <li>
            <div class="img">
              <img src="/papers/images/2023_vcip_ahfu.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">AHFu-Net: Align, Hallucinate, and Fuse Network for Missing Multimodal Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Muhammad Adi Nugroho, <b>Sangmin Woo</b>, Sumin Lee, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">VCIP 2023</span> <span style="font-weight:normal;color:#BB2222">Oral presentation</span></p>
            <p class="resources">
              [
              <a href="https://ieeexplore.ieee.org/document/10402736">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_vcip_multi.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Multi-modal Social Group Activity Recognition in Panoramic Scene</span></p>
            <p class="authors"><span style="color:#626567">Donguk Kim, Sumin Lee, <b>Sangmin Woo</b>, Jinyoung Park, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">VCIP 2023</span> </p>
            <p class="resources">
              [
              <a href="https://ieeexplore.ieee.org/document/10402675">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_cviu_cross.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Cross-Modal Alignment and Translation for Missing Modality Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Yeonju Park, <b>Sangmin Woo</b>, Sumin Lee, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">CVIU 2023</span> <span style="font-weight:normal; color:#BB2222">(IF=4.8)</span> </p>
            <p class="resources">
              [
              <a href="https://www.sciencedirect.com/science/article/pii/S1077314223001856">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_iccv_audio.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Audio-Visual Glance Network for Efficient Video Recognition</span></p>
            <p class="authors"><span style="color:#626567">Muhammad Adi Nugroho, <b>Sangmin Woo</b>, Sumin Lee, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ICCV 2023</span> <br>
              <span style="font-weight:normal; color:#BB2222">Invited Paper Talk @ CARAI Workshop</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2308.09322">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_aaai_towards.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Towards Good Practices for Missing Modality Robust Action Recognition</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Sumin Lee, Yeonju Park, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">AAAI 2023 </span> <span style="font-weight:normal;color:#BB2222">Oral presentation</span></p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2211.13916">paper</a> |
              <a href="https://github.com/sangminwoo/ActionMAE">code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_wacv_modality.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Modality Mixer for Multi-modal Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Sumin Lee, <b>Sangmin Woo</b>, Yeonju Park, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">WACV 2023</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2208.11314">paper</a>
              ]
            </p>
          </div>
        </li>

        <hr>
          <b>~ 2022</b>

        <li>
            <div class="img">
              <img src="/papers/images/2022_arxiv_explore.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span> <span class="badge badge-danger">Video</span></p>
            <p class="authors">Explore and Match: Bridging Proposal-Based and Proposal-Free with Transformer for Sentence Grounding in Videos</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Jinyoung Park, Inyong Koo, Sumin Lee, Minki Jeong, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">Arxiv 2022</span> <br>
              <span style="font-weight:normal; color:#BB2222">Finalist, 29th HumanTech Paper Award, Samsung Electronics Co., Ltd</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2201.10168">paper</a> |
              <a href="https://github.com/sangminwoo/Explore-And-Match">code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2022_tnnls_tackling.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-success">Multi-modal</span><span class="badge badge-warning">Image</span></p>
            <p class="authors">Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Junhyug Noh, Kangil Kim</span></p>
            <p class="venue"><span style="color:#626567">TNNLS 2022</span> <span style="font-weight:normal; color:#BB2222">(IF=14.2)</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2106.08543">paper</a> |
              <a href="https://github.com/sangminwoo/Local-to-Global-Interaction-Networks-SGG">code</a>
              ]
            </p>
          </div>
        </li>
        <li>
            <div class="img">
              <img src="/papers/images/2022_icip_temporal.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-warning">Image</span></p>
            <p class="authors">Temporal Flow Mask Attention for Open-Set Long-Tailed Recognition of Wild Animals in Camera-Trap Images</span></p>
            <p class="authors"><span style="color:#626567">Jeongsoo Kim, <b>Sangmin Woo</b>, Byeongjun Park, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ICIP 2022</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2208.14625">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
            <div class="img">
              <img src="/papers/images/2022_appliedsciences_impact.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-secondary">Learning</span></p>
            <p class="authors">Impact of Sentence Representation Matching in Neural Machine Translation</span></p>
            <p class="authors"><span style="color:#626567">Heeseung Jung, Kangil Kim, Jong-Hun Shin, Seung-Hoon Na, Sangkeun Jung, <b>Sangmin Woo</b></span></p>
            <p class="venue"><span style="color:#626567">Applied Sciences 2022</span> <span style="font-weight:normal; color:#BB2222">(IF=2.8)</span> </p>
            <p class="resources">
              [
              <a href="https://www.mdpi.com/2076-3417/12/3/1313">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2021_electronics_revisiting.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors"><span class="badge badge-secondary">Learning</span></p>
            <p class="authors">Revisiting Dropout: Escaping Pressure for Training Neural Networks with Multiple Costs</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Kangil Kim, Junhyug Noh, Jong-Hun Shin, and Seung-Hoon Na</span></p>
            <p class="venue"><span style="color:#626567">Electronics 2021</span> <span style="font-weight:normal; color:#BB2222">(IF=2.6)</span></p>
            <p class="resources">
              [
              <a href="https://www.mdpi.com/2079-9292/10/9/989">paper</a> |
              <a href="https://github.com/sangminwoo/Cost-Out-Multitask-Learning">code</a>
              ]
            </p>
          </div>
        </li>
        </ul>      
    </div>  
</div>
    </div>
  </section>


<section id="others" class="home-section wg-blank   " style="padding: 10px 0 10px 0;" >
<div class="container">

<div class="row">
  
    <div class="col-lg-12">
    <h1>Academic Activities</h1>
<ul>
  <li>Reviewer at CVPR (2024 ~), ICCV (2025~), ECCV (2024 ~)</li>
  <li>Reviewer at ICLR (2024 ~), NeurIPS (2024 ~), ICML (2025 ~), AAAI (2023 ~) </li>
  <li>Reviewer at IEEE TNNLS (2023 ~), TMLR (2025 ~), TIP (2025 ~)</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="awards" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Awards &amp; Honors</h1>      
<ul>
<li>Outstanding Reviewer (711/12,593), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun, 2025</li>
<li>Fianlist ($ 1,000), Qualcomm Innovation Fellowship 2024 Korea. Dec, 2024</li>
<li>Invited Paper Talk at CARAI Workshop. Center for Applied Research in Artificial Intelligence. Oct, 2023</li>
<li>Finalist, 29th HumanTech Paper Award, Samsung Electronics Co., Ltd. Dec, 2022</li>
<li>Top Award ($ 10,000), LG Electronics Robot Contest, LG Electronics Co., Ltd. Dec, 2021</li>
<li>Excellence Award ($ 500), Creative Space G A.I&IoT Makerthon, GIST. Nov, 2019</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="patents" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Patents</h1>      
<ul>
<li>Automated Visual Prompt Routing for Visual Prompt Engineering Without Access to Model Internals (US Patent)</li>
<li>Group Activity Recognition Apparatus and Method using RGB Videos and LiDAR Data (KR Patent: 10-2741168)</li>
<li>Method and Device for Inferring Dynamic Relationship between Objects in Video (KR Patent: 10-2590916)</li>
<li>Human Activity Recognition Apparatus, Human Activity Recognition Method and Computer Program for Classifying Activity Categories of User (KR Patent: 10-2515065)</li>
<li>Scene Graph Generation Apparatus (KR Patent 10-2254768)</li>
</ul>
    </div>
</div>
    </div>
  </section>

  
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script> 
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    <script>const code_highlighting = false;</script>
    <script>const isSiteThemeDark = false;</script>
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>

    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>

    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>


  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
    .
    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

<!-- Default Statcounter code for sangminwoo https://sangminwoo.github.io/ -->
<script type="text/javascript">
var sc_project=13002492; 
var sc_invisible=1; 
var sc_security="894dce66"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/13002492/0/894dce66/1/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>
