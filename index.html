<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  <meta name="author" content="Sangmin Woo">
  <meta name="description" content="Ph.D. Candidate in EE @ KAIST">
  <link rel="alternate" hreflang="en-us" href="/">
  <meta name="theme-color" content="#2962ff">
   
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
  
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="/css/academic.css">
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Sangmin Woo">
  <link rel="manifest" href="/index.webmanifest">

  <link rel="canonical" href="/">
  <meta property="twitter:card" content="summary">
  <meta property="og:site_name" content="Sangmin Woo - KAIST">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Sangmin Woo">
  <meta property="og:description" content="Ph.D. Candidate in EE @ KAIST"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "/"
}
</script>
  <title>Sangmin Woo</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Sangmin Woo</a>
    </div>
    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>

    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Sangmin Woo</a>
    </div>

    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content"> 
      <ul class="navbar-nav d-md-inline-flex">
        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#papers" data-target="#papers"><span>Publications</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#experiences" data-target="#experiences"><span>Experiences</span></a>
        </li>

      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>

    </ul>

  </div>
</nav>


<span class="js-widget-page d-none"></span>

  <section id="about" class="home-section wg-about   " style="padding: 30px 0 20px 0;" >
    <div class="container">

<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">
      <img class="avatar avatar-circle" src="/authors/admin/profile.png" alt="Avatar">
      <div class="portrait-title">
        <h2>Sangmin Woo</h2>
        <h3>Ph.D. Candidate in EE @ KAIST</h3>
      </div>
      <!-- td>
      <br>
      <a href="cv/CV_Sangmin_Oct_2023.pdf"><b>CV</b></a>
        | <a href="https://scholar.google.com/citations?user=5hvoV1UAAAAJ"><b>Google Scholar</b></a>
        | <a href="https://github.com/sangminwoo"><b>Github</b></a>
      <br>
      <br>
      </td> -->
      <ul class="network-icon" aria-hidden="true">    
        <li>
          <a href="https://scholar.google.com/citations?user=5hvoV1UAAAAJ" target="_blank" rel="noopener"><i class="ai ai-google-scholar"></i> <b>Google Scholar</b>
          </a>
        </li>  

        <li>
          <a href="https://github.com/sangminwoo" target="_blank" rel="noopener"><i class="fab fa-github"></i> <b>Github</b>
          </a>
        </li>

        <li>
          <a href="cv/CV_Sangmin_Oct_2023.pdf" target="_blank" rel="noopener"><i class="fas fa-user-circle"></i> <b>CV</b>
          </a>
        </li>

      </ul>
    </div>
  </div>
  <div class="col-12 col-lg-8">

    
<br>
<p>I am currently pursuing a Ph.D. degree in Electrical Engineering at <a href="https://kaist.ac.kr/" target="_blank" rel="noopener">KAIST</a>. In 2021, I completed an M.S. degree in Electrical Engineering and Computer Science at <a href="https://www.gist.ac.kr/" target="_blank" rel="noopener">GIST</a>. Prior to that, I obtained a B.S. degree in Electrical Engineering from <a href="https://www.knu.ac.kr/" target="_blank" rel="noopener">KNU</a> in 2019.</p>
    
<p>Humans are inherently multi-modal multi-task learners, with vision playing a pivotal role in shaping our understanding of the world.
I am passionate about bridging the gap between machine perception and human-level understanding by harnessing the potential of <b>multi-modal multi-task learning</b>.</p>

<p>I thrive on creative challenges and enjoy building strong relationships along the way. Explore my academic journey below, and contact me directly to learn more.</p>

    <div class="row">

      <div class="col-md-6">
        <h3>Contact</h3>
        <ul class="ul-contact fa-ul">
          <li>          
            <i class="fa-li fas fa-envelope"></i>
            <div class="description">
              <p class="course">smwoo95 [at] kaist.ac.kr</p>
              <p class="course">shmwoo9395 [at] gmail.com</p>
            </div>
          </li>
          <li>  
            <i class="fa-li fas fa-map-marker"></i>
            <div class="description">
              <p class="course">291, Daehak-ro, Yuseong-gu, Daejeon, Republic of Korea 34141</p>
            </div> 
          </li>
          <!-- <li>  
            <i class="fa-li fas fa-phone-alt"></i>
            <div class="description">
              <p class="course">(&#43;82)-42-350-7521</p>
            </div> 
          </li>  -->         
        </ul>        
      </div>

      <div class="col-md-6">
        <h3>Education</h3>
        <ul class="ul-edu fa-ul">
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Ph.D. in EE, KAIST, 2025 (Expected)</p>
            </div>
          </li>
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">M.S. in EECS, GIST, 2021</p>
              <p class="institution">on "Learning to Detect Visual Relationships in Images and Videos"</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">B.S. in EE, KNU, 2019</p>
            </div>
          </li>

        </ul>
      </div>
    </div>
  </div>
</div>

    </div>
  </section>


  <section id="papers" class="home-section wg-papers   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      
<div class="row">
  
    <div class="col-lg-12">
      <h1>Publications</h1>
    </div>
    <div class="row">
        <ul class="ul-papers">
        <hr>
          <b>2024</b>
        <!-- <li>
            <div class="img">
              <img src="/papers/images/2024_cvpr_multiview.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Single-view 3D Reconstruction with Multi-view Consistency and Unseen-view Diversity</span></p>
            <p class="authors"><span style="color:#626567">Byeongjun Park*, <b>Sangmin Woo*</b>, Hyojun Go*, Jinyoung Kim*, Changick Kim (*Equal Contribution)</span></p>
            <p class="venue"><span style="color:#626567">Arxiv (under review in CVPR 2024)</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2311.10000">paper</a>
              ]
            </p>
          </div>
        </li> -->

        <li>
            <div class="img">
              <img src="/papers/images/2024_wacv_sketch.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Sketch-based Video Object Localization</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, So-Yeong Jeon, Jinyoung Park, Minji Son, Sumin Lee, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">WACV 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2304.00450">paper</a>
              ]
            </p>
          </div>
        </li>

        <hr>
          <b>2023</b>
        <li>
            <div class="img">
              <img src="/papers/images/2024_iclr_denoising.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Denoising Task Routing for Diffusion Models</span></p>
            <p class="authors"><span style="color:#626567">Byeongjun Park*, <b>Sangmin Woo*</b>, Hyojun Go*, Jinyoung Kim*, Changick Kim (*Equal Contribution)</span></p>
            <p class="venue"><span style="color:#626567">under review in ICLR 2024</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2310.07138">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_vcip_multi.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Multi-modal Social Group Activity Recognition in Panoramic Scene</span></p>
            <p class="authors"><span style="color:#626567">Donguk Kim, Sumin Lee, <b>Sangmin Woo</b>, Jinyoung Park, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">VCIP 2023</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2310.20000">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_vcip_ahfu.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">AHFu-Net: Align, Hallucinate, and Fuse Network for Missing Multimodal Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Muhammad Adi Nugroho, <b>Sangmin Woo</b>, Sumin Lee, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">VCIP 2023 (Oral)</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2310.20000">paper</a>
              ]
            </p>
          </div>
        </li>


        <li>
            <div class="img">
              <img src="/papers/images/2023_cviu_cross.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Cross-Modal Alignment and Translation for Missing Modality Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Yeonju Park, <b>Sangmin Woo</b>, Sumin Lee, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">CVIU 2023</span> </p>
            <p class="resources">
              [
              <a href="https://www.sciencedirect.com/science/article/pii/S1077314223001856">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_tip_modality.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Modality Mixer Exploiting Complementary Information for Multi-modal Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Sumin Lee, <b>Sangmin Woo</b>, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">under review in TIP</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2208.11314">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_iccv_audio.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Audio-Visual Glance Network for Efficient Video Recognition</span></p>
            <p class="authors"><span style="color:#626567">Muhammad Adi Nugroho, <b>Sangmin Woo</b>, Sumin Lee, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ICCV 2023</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2308.09322">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_aaai_towards.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Towards Good Practices for Missing Modality Robust Action Recognition</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Sumin Lee, Yeonju Park, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">AAAI 2023 (Oral)</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2211.13916">paper</a> |
              <a href="https://github.com/sangminwoo/ActionMAE">code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_wacv_modality.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Modality Mixer for Multi-modal Action Recognition</span></p>
            <p class="authors"><span style="color:#626567">Sumin Lee, <b>Sangmin Woo</b>, Yeonju Park, Muhammad Adi Nugroho, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">WACV 2023</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2208.11314">paper</a>
              ]
            </p>
          </div>
        </li>

        <hr>
          <b>2022</b>

        <li>
            <div class="img">
              <img src="/papers/images/2022_arxiv_explore.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Explore and Match: Bridging Proposal-Based and Proposal-Free with Transformer for Sentence Grounding in Videos</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Jinyoung Park, Inyong Koo, Sumin Lee, Minki Jeong, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">under review in TMM</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2201.10168">paper</a> |
              <a href="https://github.com/sangminwoo/Explore-And-Match">code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2022_tnnls_tackling.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Junhyug Noh, Kangil Kim</span></p>
            <p class="venue"><span style="color:#626567">TNNLS 2022</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2106.08543">paper</a> |
              <a href="https://github.com/sangminwoo/Local-to-Global-Interaction-Networks-SGG">code</a>
              ]
            </p>
          </div>
        </li>
        <li>
            <div class="img">
              <img src="/papers/images/2022_icip_temporal.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Temporal Flow Mask Attention for Open-Set Long-Tailed Recognition of Wild Animals in Camera-Trap Images</span></p>
            <p class="authors"><span style="color:#626567">Jeongsoo Kim, <b>Sangmin Woo</b>, Byeongjun Park, Changick Kim</span></p>
            <p class="venue"><span style="color:#626567">ICIP 2022</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2208.14625">paper</a>
              ]
            </p>
          </div>
        </li>
        <li>
            <div class="img">
              <img src="/papers/images/2022_appliedsciences_impact.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Impact of Sentence Representation Matching in Neural Machine Translation</span></p>
            <p class="authors"><span style="color:#626567">Heeseung Jung, Kangil Kim, Jong-Hun Shin, Seung-Hoon Na, Sangkeun Jung, <b>Sangmin Woo</b></span></p>
            <p class="venue"><span style="color:#626567">Applied Sciences 2022</span> </p>
            <p class="resources">
              [
              <a href="https://www.mdpi.com/2076-3417/12/3/1313">paper</a>
              ]
            </p>
          </div>
        </li>

        <hr>
          <b>2021</b>

        <li>
            <div class="img">
              <img src="/papers/images/2021_arxiv_what.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Junhyug Noh, Kangil Kim</span></p>
            <p class="venue"><span style="color:#626567">under review in ESWA</span> </p>
            <p class="resources">
              [
              <a href="https://arxiv.org/abs/2107.07154">paper</a> |
              <a href="https://github.com/sangminwoo/Temporal-Span-Proposal-Network-VidVRD">code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2021_electronics_revisiting.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="authors">Revisiting Dropout: Escaping Pressure for Training Neural Networks with Multiple Costs</span></p>
            <p class="authors"><span style="color:#626567"><b>Sangmin Woo</b>, Kangil Kim, Junhyug Noh, Jong-Hun Shin, and Seung-Hoon Na</span></p>
            <p class="venue"><span style="color:#626567">Electronics 2021</span> </p>
            <p class="resources">
              [
              <a href="https://www.mdpi.com/2079-9292/10/9/989">paper</a> |
              <a href="https://github.com/sangminwoo/Cost-Out-Multitask-Learning">code</a>
              ]
            </p>
          </div>
        </li>
        </ul>      
    </div>  
</div>
    </div>
  </section>


<section id="experiences" class="home-section wg-experiences   " style="padding: 10px 0 10px 0;" >
<div class="container">

<div class="row">
  
    <div class="col-lg-12">
    <h1>Research Experiences</h1>
<ul>

<li> <div style="float:left"><b>Computational Intelligence Lab @ KAIST</b> </div><div style="float:right">Sep 2021 - Present</div> <br>
    Research Assistant<br>
</li>
<li> <div style="float:left"><b>Robot Vision Team @ NAVER LABS</b> </div><div style="float:right">Apr 2023 - Aug 2023</div> <br>
    Research Intern<br>
</li>
<li> <div style="float:left"><b>Intelligence Representation and Reasoning Lab @ GIST</b> </div><div style="float:right">Sep 2019 - Aug 2021</div> <br>
    Research Assistant<br>
</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="activities" class="home-section wg-blank   " style="padding: 10px 0 10px 0;" >
<div class="container">

<div class="row">
  
    <div class="col-lg-12">
    <h1>Academic Activities</h1>
<ul>
  <li>Reviewer at AAAI (2023, 2024)</li>
  <li>Reviewer at IEEE TNNLS, IEEE TCSVT</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="awards" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Awards &amp; Honors</h1>      
<ul>
<li>Talk at CARAI Workshop. Center for Applied Research in Artificial Intelligence. Oct, 2023</li>
<li>Finalist, 29th HumanTech Paper Award, Samsung Electronics Co., Ltd. Dec, 2022</li>
<li>Top Award ($ 10,000), LG Electronics Robot Contest, LG Electronics Co., Ltd. Dec, 2021</li>
<li>Excellence Award ($ 500), Creative Space G A.I&IoT Makerthon, GIST. Nov, 2019</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="patents" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Patents</h1>      
<ul>
<li>Method and Appratus for Human Activity Recognition using Accelerometer and Gyroscope Sensors (KR Patent Application: 10-2022-0094911)</li>
<li>Method and Device for Inferring Dynamic Relationship between Objects in Video (KR Patent Application: 10-2021-0125704)</li>
<li>Scene Graph Generation Apparatus (KR Patent 10-2254-7680000)</li>
</ul>
    </div>
</div>
    </div>
  </section>

  
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script> 
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    <script>const code_highlighting = false;</script>
    <script>const isSiteThemeDark = false;</script>
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>

    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>

    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>


  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
    .
    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>


